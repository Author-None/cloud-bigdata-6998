\documentclass[12pt, a4paper]{article}

\usepackage{geometry}

% Comment the following lines to use the default Computer Modern font
\usepackage{helvet}
\renewcommand\familydefault{\sfdefault} 
\usepackage[T1]{fontenc}

%hyperlink
\usepackage[colorlinks=true, citecolor=red]{hyperref}

%enumerate
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0pt, topsep=0pt}
\setlist[itemize]{itemsep=0pt, topsep=0pt}

%tex formatting
\usepackage{textcomp}

%headers and footers
\usepackage{fancyhdr}

%title formatting
\usepackage{titlesec}

\usepackage[parfill]{parskip}

% Avoid line on top of pages
\renewcommand{\headrulewidth}{0pt}

% Set your name here
\def\name{Kunal Baweja}

% Set university id here
\def\uni{kb2896}

%Set assignment title here
\def\assignment{Homework-1: Summary of The Google File System}

%Set subject name here
\def\subject{COMS E6998 Cloud Computing \& Big Data (Fall 2016)}

\titlespacing{\section}{0pt}{12pt}{0pt}

\geometry{
        headheight=15pt,
        left=0.6in,
        top=0.9in,
        right=0.6in
}


% Customize pageheaders
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\assignment}
\fancyfoot[L]{\name \hspace{0.25cm} \uni}
\fancyfoot[R]{\thepage}
\thispagestyle{empty}

% Custom section fonts
\titleformat*{\section}{\large\bfseries}
\setcounter{secnumdepth}{0}

% Don't indent paragraphs.
\setlength{\parindent}{0pt}

\begin{document}

% Assignment Heading in Center
\begin{center}
    {\Large \assignment}
    \vspace{0.1em}
\end{center}

Name: {\name}\\
UNI: {\uni}\\
{\subject} \hfill {\today}

%horizontal line
\rule{\textwidth}{0.2pt}

\section*{Introduction}
This article summarizes the various aspects, design principles and goals of \textit{Google File System}\cite{Ghemawat:2003:GFS:945445.945450} presented by Sanjay Ghemawat, Howard obioff and Shun-Tak Leung, as researchers at Google.\par

The \textit{Google File System(GFS)}\cite{Ghemawat:2003:GFS:945445.945450} is a scalable distributed file system designed for large, distributed and data intensive applications. It provides efficient performance on large number of clients and storage machines and also offers fault tolerance by constant monitoring, replication of crucial data, and fast and automatic recovery. In addition, \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} is capable of running on inexpensive commodity hardware. It mininmizes the metadata storage for file system and separates data flow from control flow to maximize network bandwidth usage. The relaxed consistency model of \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} simplifies file system design greatly by excluding factors such as symlinks etc in comparison to traditional file systems and also provides crucial guarantees in terms of data availability to applications using \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450}.\par

\textbf{Design:} The data intensive distributed applications require guarantees in terms of data availability, integrity, reduced network latency, file system consistency and efficient metadata management. \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} tries to address these concerns by making following design considerations:

\begin{enumerate}
\item Component failures are considered as a norm rather than an exception to tackle failure of hardware components.

\item The average size of files stoered on \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} is assumed to be of multiple gigabytes.

\item Most files are modified by appending new data, rather than overwriting existing data and once written, all files are read-only in a sequential manner.

\item API designs for \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} and applications intended to run over it have been optimised to support high read and write throughputs along with multiple clients concurrently appending to a file. This is supported with \textit{atomicity of writes} guarantee provided by relaxed consistency model for \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450}.

\item High sustained bandwidth is more important than low latency as it's assumed that most applications prioritize processing large amounts of data than quick response times.
\end{enumerate}

\textbf{File System Interface:} Apart from the standard file system operations, \textit{create, delete, open, close, read and write}, \textit{Google File System}\cite{Ghemawat:2003:GFS:945445.945450} provides two new operations:

\begin{itemize}
\item The \textit{snapshot} operation facilitates users by creating a copy of a file or directory tree at low cost, sometimes even receursively if required by user.

\item The \textit{record append} operation allows multiple clients to append data concurrently to a file while guaranteeing atomicity of operations.
\end{itemize}

\textbf{Architecture:} A \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} cluster consists of single \textit{master} and multiple \textit{chunkservers} which are accessed concurrently by multiple clients. Files are divided into fixed-size \textit{chunks}. Each \textit{chunk} is identified by an immutable and globally unique 64 bit \textit{chunk handle} assigned by the master at the time of chunk creation. Chunkservers store chunks on local disks as Linux files and read or write chunk data specified by a chunk handle and byte range. For reliability, each chunk is replicated on multiple chunkservers and the metadata of the whole file system including information about chunks is maintained by the master node. Clients interact with the master node for metadata related operations but all data-bearing communication is directly handled by chunkservers, as delegated by the master.\par

The single master node, specifically handles all filesystem metadata operations and records operation logs.It has minimal or no involvment in reading and writing data, thus preventing it from becoming a bottleneck in I/O operations. The client contacts the master node for chunkserver information for read/write operations. The master replies with the corresponding chunk handle and locations of the replicas, client caches this response using file name and chunk index as the key and then directly contacts relevant chunkservers in the future for data operations.\par

\textbf{Fault Tolerance and Diagnosis:} \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} applies an array of measures to incorporate fault tolerance against possible failure of system components:

\begin{enumerate}
        \item \textbf{High Availability} - \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} employs \textit{fast recovery} wherein it doesn't distinguishes between normal and abnormal termination of master or chunkserver, hence enabling them to restore their state and start within a few seconds irrespective of cause of shutting down. It also implements \textit{chunk replication} to deal with situations like disk failures, network failure or server failure etc, so in case one or more chunkservers become unavailable, there are replicas available to take up the requests for data on their behalf. \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} also replicates metadata stored in \textit{master} node to safeguard against failure of master node.

        \item \textbf{Data Integrity}: Each chunkserver uses checksumming to detect corruption of stored data. To confirm integrity of data across multiple replicas, the chunkservers obtain checksum(e.g., MD5) which is compared against the checksum values of othe chunkservers containing the replicas of the same chunk of data to detect anamolies. This mechanism prevents against data integrity failures due to component failures or read-write failures.

        \item \textbf{Diagnostic Tools}: \textit{GFS}\cite{Ghemawat:2003:GFS:945445.945450} servers generate diagnostic logs that record significant events and prove helpful in problem isolation, debugging and performance analysis. The performance effect of this logging is minimal whereas benefits are huge, as controlling and observing a cluster of thousands of nodes is not an easy task without proper logging mechanisms.
\end{enumerate}

\bibliography{945450} 
\bibliographystyle{ieeetr}

\end{document}